{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7d35304-720c-468d-a3fa-3a60adb01dff",
   "metadata": {},
   "source": [
    "### Importing all the required Liberaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faa47051-1a17-43f8-9d81-ce0d08a5e119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lxml import etree as et"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ffdb21-4912-4f01-a265-a3d60899f61f",
   "metadata": {},
   "source": [
    "### 1) Write a python program to display all the header tags from wikipedia.org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "891c8d78-b85a-4299-ad21-62817cfddb9c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Did you know ...\n",
      "List all the header tags :\n",
      "\n",
      "<h1 class=\"firstHeading mw-first-heading\" id=\"firstHeading\" style=\"display: none\"><span class=\"mw-page-title-main\">Main Page</span></h1>\n",
      "\n",
      "<h1><span class=\"mw-headline\" id=\"Welcome_to_Wikipedia\">Welcome to <a href=\"/wiki/Wikipedia\" title=\"Wikipedia\">Wikipedia</a></span></h1>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfa-h2\"><span id=\"From_today.27s_featured_article\"></span><span class=\"mw-headline\" id=\"From_today's_featured_article\">From today's featured article</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-dyk-h2\"><span class=\"mw-headline\" id=\"Did_you_know_...\">Did you know ...</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-itn-h2\"><span class=\"mw-headline\" id=\"In_the_news\">In the news</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-otd-h2\"><span class=\"mw-headline\" id=\"On_this_day\">On this day</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfp-h2\"><span id=\"Today.27s_featured_picture\"></span><span class=\"mw-headline\" id=\"Today's_featured_picture\">Today's featured picture</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-other\"><span class=\"mw-headline\" id=\"Other_areas_of_Wikipedia\">Other areas of Wikipedia</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-sister\"><span id=\"Wikipedia.27s_sister_projects\"></span><span class=\"mw-headline\" id=\"Wikipedia's_sister_projects\">Wikipedia's sister projects</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-lang\"><span class=\"mw-headline\" id=\"Wikipedia_languages\">Wikipedia languages</span></h2>\n",
      "\n",
      "<h2>Navigation menu</h2>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-personal-label\">\n",
      "<span class=\"vector-menu-heading-label\">Personal tools</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-namespaces-label\">\n",
      "<span class=\"vector-menu-heading-label\">Namespaces</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-views-label\">\n",
      "<span class=\"vector-menu-heading-label\">Views</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-navigation-label\">\n",
      "<span class=\"vector-menu-heading-label\">Navigation</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-interaction-label\">\n",
      "<span class=\"vector-menu-heading-label\">Contribute</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-tb-label\">\n",
      "<span class=\"vector-menu-heading-label\">Tools</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-coll-print_export-label\">\n",
      "<span class=\"vector-menu-heading-label\">Print/export</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-wikibase-otherprojects-label\">\n",
      "<span class=\"vector-menu-heading-label\">In other projects</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-lang-label\">\n",
      "<span class=\"vector-menu-heading-label\">Languages</span>\n",
      "</h3>\n",
      "Main Page\n",
      "Welcome to Wikipedia\n",
      "From today's featured article\n",
      "Did you know ...\n",
      "In the news\n",
      "On this day\n",
      "Today's featured picture\n",
      "Other areas of Wikipedia\n",
      "Wikipedia's sister projects\n",
      "Wikipedia languages\n",
      "Navigation menu\n",
      "\n",
      "Personal tools\n",
      "\n",
      "\n",
      "Namespaces\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "Navigation\n",
      "\n",
      "\n",
      "Contribute\n",
      "\n",
      "\n",
      "Tools\n",
      "\n",
      "\n",
      "Print/export\n",
      "\n",
      "\n",
      "In other projects\n",
      "\n",
      "\n",
      "Languages\n",
      "\n"
     ]
    }
   ],
   "source": [
    "page = requests.get('https://en.wikipedia.org/wiki/Main_Page')\n",
    "print(page.status_code)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "print(soup.find_all('h2', class_ = 'mp-h2')[1].get_text())\n",
    "headers = soup.find_all(['h1', 'h2','h3','h4','h5','h6'])\n",
    "print('List all the header tags :', *headers, sep='\\n\\n')\n",
    "for i in headers:\n",
    "    print(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dbedf5-24d8-4244-bfd5-3116555f1c48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "484a04a3-f171-4ffa-bee1-a9d0042126dd",
   "metadata": {},
   "source": [
    "### 2. Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. name, rating, year of release) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a6aefbf-a1a4-42ab-b366-e2d923020d3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "imbd = requests.get('https://www.imdb.com/search/title/?groups=top_100&sort=user_rating,desc')  # Make a request to a web page with requests module    \n",
    "print(imbd.status_code) #HTTP response status codes indicate whether a specific HTTP request has been successfully completed here,‎200 Successful responses ‎418 I'm a teapot · ‎400 Bad Request · ‎422 Unprocessable Entity\n",
    "soup = BeautifulSoup(imbd.text, 'html.parser') # pulling data out of HTML and XML files with BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4eaa67-a9e7-4af1-8460-ca53a8d29f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63a5f3b0-55ce-4ac1-8a33-d63d68323d26",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Year</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>(1994)</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Godfather</td>\n",
       "      <td>(1972)</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>(2008)</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Lord of the Rings: The Return of the King</td>\n",
       "      <td>(2003)</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Schindler's List</td>\n",
       "      <td>(1993)</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>North by Northwest</td>\n",
       "      <td>(1959)</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Vertigo</td>\n",
       "      <td>(1958)</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Singin' in the Rain</td>\n",
       "      <td>(1952)</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Citizen Kane</td>\n",
       "      <td>(1941)</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>M - Eine Stadt sucht einen Mörder</td>\n",
       "      <td>(1931)</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Title    Year Rating\n",
       "0                        The Shawshank Redemption  (1994)    9.3\n",
       "1                                   The Godfather  (1972)    9.2\n",
       "2                                 The Dark Knight  (2008)    9.0\n",
       "3   The Lord of the Rings: The Return of the King  (2003)    9.0\n",
       "4                                Schindler's List  (1993)    9.0\n",
       "..                                            ...     ...    ...\n",
       "45                             North by Northwest  (1959)    8.3\n",
       "46                                        Vertigo  (1958)    8.3\n",
       "47                            Singin' in the Rain  (1952)    8.3\n",
       "48                                   Citizen Kane  (1941)    8.3\n",
       "49              M - Eine Stadt sucht einen Mörder  (1931)    8.3\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year = soup.find_all('span', class_ = 'lister-item-year text-muted unbold')\n",
    "years = [x.text for x in year]\n",
    "year = pd.DataFrame(years, columns = ['Year'])\n",
    "\n",
    "rating = soup.find_all('div', class_ = 'ratings-imdb-rating')\n",
    "rating = [x.text.strip() for x in rating]\n",
    "rating = pd.DataFrame(rating, columns= ['Rating'])\n",
    "\n",
    "results = soup.find_all('h3')\n",
    "title = [x.text.strip() for x in results]\n",
    "#pd.DataFrame(data['Title'].values.tolist(), index=data.index)\n",
    "Title = pd.DataFrame(title, columns = ['Title'])\n",
    "\n",
    "Title = Title['Title'].str.split('\\n', expand=True)\n",
    "Title.rename(columns = {1 : 'Title'}, inplace = True)         #rename column name   \n",
    "Title = Title.iloc[:50,1:2]                                   #slicing the data with iloc\n",
    "\n",
    "data =  pd.concat([Title, year, rating], axis=1, join=\"inner\")    #aggregation into 1\n",
    "\n",
    "\n",
    "#Scraping Page 2\n",
    "\n",
    "imbd_2 = requests.get('https://www.imdb.com/search/title/?groups=top_100&sort=user_rating,desc&start=51&ref_=adv_nxt')\n",
    "#print(imbd_2.status_code)\n",
    "soup_2 = BeautifulSoup(imbd_2.text, 'html.parser')\n",
    "year = soup_2.find_all('span', class_ = 'lister-item-year text-muted unbold')\n",
    "years = [x.text for x in year]\n",
    "year = pd.DataFrame(years, columns = ['Year'])\n",
    "rating = soup_2.find_all('div', class_ = 'ratings-imdb-rating')\n",
    "rating = [x.text.strip() for x in rating]\n",
    "rating = pd.DataFrame(rating, columns= ['Rating'])\n",
    "results = soup_2.find_all('h3')\n",
    "title = [x.text.strip() for x in results]\n",
    "#pd.DataFrame(data['Title'].values.tolist(), index=data.index)\n",
    "Title = pd.DataFrame(title, columns = ['Title'])\n",
    "Title = Title['Title'].str.split('\\n', expand=True)\n",
    "Title.rename(columns = {1 : 'Title'}, inplace = True)    \n",
    "Title = Title.iloc[:50,1:2]\n",
    "\n",
    "data_2 =  pd.concat([Title, year, rating], axis=1, join=\"inner\")\n",
    "Top_100_movies = pd.concat([data, data_2], axis = 0)\n",
    "Top_100_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f825f4ee-479f-4535-bdbc-dfbb1e584209",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4115a155-9a34-4905-b931-b3106e601068",
   "metadata": {},
   "source": [
    "#### 3) Write a python program to display IMDB’s Top rated 100 Indian movies’ data (i.e. name, rating, year of release) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "39fcde52-afe8-402b-9820-837427d2eaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "imbd_hindi = requests.get('https://www.imdb.com/list/ls056092300/?sort=user_rating,desc&st_dt=&mode=detail&page=1')  # Make a request to a web page with requests module    \n",
    "print(imbd.status_code) #HTTP response status codes indicate whether a specific HTTP request has been successfully completed here,‎200 Successful responses ‎418 I'm a teapot · ‎400 Bad Request · ‎422 Unprocessable Entity\n",
    "soup = BeautifulSoup(imbd_hindi.text, 'html.parser') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "726a85ce-2e1f-4961-8f88-2fa43f8ee80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = pd.DataFrame([x.text for x in soup.find_all('h3')], columns = ['Name'])\n",
    "name = name['Name'].str.split('\\n', expand=True).iloc[:100,2:3]\n",
    "name.rename(columns = {2: 'Name'}, inplace = True )\n",
    "\n",
    "rating = pd.DataFrame([x.text.split() for x in soup.find_all('div', class_ = 'ipl-rating-star small' )], columns = ['Rating'])\n",
    "\n",
    "year = pd.DataFrame([x.text for x in soup.find_all('span', class_ = 'lister-item-year text-muted unbold')], columns = ['Year_of_Release'])\n",
    "\n",
    "Top_100_Indian = pd.concat([name, rating, year], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8c7e5a22-78c4-435e-8393-f544316cbf49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Year_of_Release</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sagara Sangamam</td>\n",
       "      <td>8.8</td>\n",
       "      <td>(1983)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Goopy Gyne Bagha Byne</td>\n",
       "      <td>8.7</td>\n",
       "      <td>(1969)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nayakan</td>\n",
       "      <td>8.6</td>\n",
       "      <td>(1987)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pushpaka Vimana</td>\n",
       "      <td>8.6</td>\n",
       "      <td>(1987)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Apur Sansar</td>\n",
       "      <td>8.5</td>\n",
       "      <td>(1959)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Jis Desh Men Ganga Behti Hai</td>\n",
       "      <td>7.2</td>\n",
       "      <td>(1960)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Vanaja</td>\n",
       "      <td>7.2</td>\n",
       "      <td>(2006)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Enthiran</td>\n",
       "      <td>7.1</td>\n",
       "      <td>(2010)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Chandni</td>\n",
       "      <td>6.7</td>\n",
       "      <td>(1989)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Neecha Nagar</td>\n",
       "      <td>6.7</td>\n",
       "      <td>(1946)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Name Rating Year_of_Release\n",
       "0                Sagara Sangamam    8.8          (1983)\n",
       "1          Goopy Gyne Bagha Byne    8.7          (1969)\n",
       "2                        Nayakan    8.6          (1987)\n",
       "3                Pushpaka Vimana    8.6          (1987)\n",
       "4                    Apur Sansar    8.5          (1959)\n",
       "..                           ...    ...             ...\n",
       "95  Jis Desh Men Ganga Behti Hai    7.2          (1960)\n",
       "96                        Vanaja    7.2          (2006)\n",
       "97                      Enthiran    7.1          (2010)\n",
       "98                       Chandni    6.7          (1989)\n",
       "99                  Neecha Nagar    6.7          (1946)\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Top_100_Indian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3c2d74-bdcf-40db-a154-f47b01caa809",
   "metadata": {},
   "source": [
    "#### 4) Write s python program to display list of respected former presidents of India(i.e. Name , Term of office) from https://presidentofindia.nic.in/former-presidents.htm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8baae0cc-b542-48df-a170-bb0fa89d0a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "president = requests.get('https://presidentofindia.nic.in/former-presidents.htm')\n",
    "print(president.status_code)\n",
    "soup= BeautifulSoup(president.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ca011128-2079-4d92-9f99-89fbefae4829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Terms_of_Office</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shri Ram Nath Kovind (birth - 1945)</td>\n",
       "      <td>Term of Office: 25 July, 2017 to 25 July, 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shri Pranab Mukherjee (1935-2020)</td>\n",
       "      <td>Term of Office: 25 July, 2012 to 25 July, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Smt Pratibha Devisingh Patil (birth - 1934)</td>\n",
       "      <td>Term of Office: 25 July, 2007 to 25 July, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DR. A.P.J. Abdul Kalam (1931-2015)</td>\n",
       "      <td>Term of Office: 25 July, 2002 to 25 July, 2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shri K. R. Narayanan (1920 - 2005)</td>\n",
       "      <td>Term of Office: 25 July, 1997 to 25 July, 2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Dr Shankar Dayal Sharma (1918-1999)</td>\n",
       "      <td>Term of Office: 25 July, 1992 to 25 July, 1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Shri R Venkataraman (1910-2009)</td>\n",
       "      <td>Term of Office: 25 July, 1987 to 25 July, 1992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Giani Zail Singh (1916-1994)</td>\n",
       "      <td>Term of Office: 25 July, 1982 to 25 July, 1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Shri Neelam Sanjiva Reddy (1913-1996)</td>\n",
       "      <td>Term of Office: 25 July, 1977 to 25 July, 1982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dr. Fakhruddin Ali Ahmed (1905-1977)</td>\n",
       "      <td>Term of Office: 24 August, 1974 to 11 February...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Shri Varahagiri Venkata Giri (1894-1980)</td>\n",
       "      <td>Term of Office: 3 May, 1969 to 20 July, 1969 a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Dr. Zakir Husain (1897-1969)</td>\n",
       "      <td>Term of Office: 13 May, 1967 to 3 May, 1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Dr. Sarvepalli Radhakrishnan (1888-1975)</td>\n",
       "      <td>Term of Office: 13 May, 1962 to 13 May, 1967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Dr. Rajendra Prasad (1884-1963)</td>\n",
       "      <td>Term of Office: 26 January, 1950 to 13 May, 1962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Name  \\\n",
       "0           Shri Ram Nath Kovind (birth - 1945)   \n",
       "1             Shri Pranab Mukherjee (1935-2020)   \n",
       "2   Smt Pratibha Devisingh Patil (birth - 1934)   \n",
       "3            DR. A.P.J. Abdul Kalam (1931-2015)   \n",
       "4            Shri K. R. Narayanan (1920 - 2005)   \n",
       "5           Dr Shankar Dayal Sharma (1918-1999)   \n",
       "6               Shri R Venkataraman (1910-2009)   \n",
       "7                  Giani Zail Singh (1916-1994)   \n",
       "8         Shri Neelam Sanjiva Reddy (1913-1996)   \n",
       "9          Dr. Fakhruddin Ali Ahmed (1905-1977)   \n",
       "10     Shri Varahagiri Venkata Giri (1894-1980)   \n",
       "11                 Dr. Zakir Husain (1897-1969)   \n",
       "12     Dr. Sarvepalli Radhakrishnan (1888-1975)   \n",
       "13              Dr. Rajendra Prasad (1884-1963)   \n",
       "\n",
       "                                      Terms_of_Office  \n",
       "0     Term of Office: 25 July, 2017 to 25 July, 2022   \n",
       "1     Term of Office: 25 July, 2012 to 25 July, 2017   \n",
       "2     Term of Office: 25 July, 2007 to 25 July, 2012   \n",
       "3     Term of Office: 25 July, 2002 to 25 July, 2007   \n",
       "4      Term of Office: 25 July, 1997 to 25 July, 2002  \n",
       "5      Term of Office: 25 July, 1992 to 25 July, 1997  \n",
       "6      Term of Office: 25 July, 1987 to 25 July, 1992  \n",
       "7      Term of Office: 25 July, 1982 to 25 July, 1987  \n",
       "8      Term of Office: 25 July, 1977 to 25 July, 1982  \n",
       "9   Term of Office: 24 August, 1974 to 11 February...  \n",
       "10  Term of Office: 3 May, 1969 to 20 July, 1969 a...  \n",
       "11        Term of Office: 13 May, 1967 to 3 May, 1969  \n",
       "12       Term of Office: 13 May, 1962 to 13 May, 1967  \n",
       "13   Term of Office: 26 January, 1950 to 13 May, 1962  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "Name = pd.DataFrame([i.text.strip() for i in soup.find_all('h3')], columns = ['Name'])\n",
    "\n",
    "term = pd.DataFrame([x.text.strip() for x in soup.find_all('div', class_ = 'presidentListing')], columns = ['Terms'])\n",
    "term = term['Terms'].str.split('\\n', expand=True).iloc[:,1:2].rename(columns = {1: 'Terms_of_Office'})\n",
    "\n",
    "Indian_president = pd.concat([Name, term], axis=1, join=\"inner\")\n",
    "Indian_president"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8c6214cf-8297-443f-a693-fc5e299b533d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc91991-c5d6-4596-acc3-f99d2ae1b6a2",
   "metadata": {},
   "source": [
    "### 5) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "* a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    "* b) Top 10 ODI Batsmen along with the records of their team and rating.\n",
    "* c) Top 10 ODI bowlers along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "4b7ced72-86f6-409d-88ff-0dafb5f355de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "odi = requests.get('https://www.icc-cricket.com/rankings/mens/player-rankings/odi')\n",
    "\n",
    "print(odi.status_code)\n",
    "soup = BeautifulSoup(odi.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf81868a-6995-443a-8ee3-b68d3cc7c0fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "312bcda2-9ee6-4640-ba80-271784d3de25",
   "metadata": {},
   "source": [
    "#### Top 10 Batsmen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "870e71c0-c4b3-4e14-bca6-e08f5435f247",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = pd.DataFrame([x.text for x in soup.find_all('td', class_ = 'table-body__cell name')],columns = ['Player']).iloc[:9]\n",
    "country = pd.DataFrame([x.text for x in soup.find_all('span', class_ = 'table-body__logo-text')],columns = ['Country']).iloc[:9]\n",
    "rating = pd.DataFrame([x.text for x in soup.find_all('td', class_ = 'table-body__cell u-text-right rating')],columns = ['Rating']).iloc[:9]\n",
    "\n",
    "\n",
    "name = name['Player'].str.split('\\n',expand=True).iloc[:,1:2].rename(columns = {1: 'Player'})\n",
    "mens_player = pd.concat([name, country, rating], axis = 1)\n",
    "\n",
    "name = pd.DataFrame([x.text for x in soup.find_all('div', class_ = 'rankings-block__banner--player-info')],columns = ['Player']).iloc[:1]\n",
    "nm = name['Player'].str.split('\\n', expand = True).rename(columns = {1:'Player', 4:'Country', 5:'Rating'})\n",
    "nm.drop(columns = [2, 3, 6, 7, 0], inplace = True)\n",
    "\n",
    "odi_mens = pd.concat([nm, mens_player] , axis = 0)\n",
    "odi_mens = odi_mens.set_index('Player')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "fb55992d-d047-4c89-be0f-99c3210a468d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Player</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Babar Azam</th>\n",
       "      <td>PAK</td>\n",
       "      <td>890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Imam-ul-Haq</th>\n",
       "      <td>PAK</td>\n",
       "      <td>779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rassie van der Dussen</th>\n",
       "      <td>SA</td>\n",
       "      <td>766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quinton de Kock</th>\n",
       "      <td>SA</td>\n",
       "      <td>759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>David Warner</th>\n",
       "      <td>AUS</td>\n",
       "      <td>747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Steve Smith</th>\n",
       "      <td>AUS</td>\n",
       "      <td>719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jonny Bairstow</th>\n",
       "      <td>ENG</td>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Virat Kohli</th>\n",
       "      <td>IND</td>\n",
       "      <td>707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rohit Sharma</th>\n",
       "      <td>IND</td>\n",
       "      <td>704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kane Williamson</th>\n",
       "      <td>NZ</td>\n",
       "      <td>701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Country Rating\n",
       "Player                              \n",
       "Babar Azam                PAK    890\n",
       "Imam-ul-Haq               PAK    779\n",
       "Rassie van der Dussen      SA    766\n",
       "Quinton de Kock            SA    759\n",
       "David Warner              AUS    747\n",
       "Steve Smith               AUS    719\n",
       "Jonny Bairstow            ENG    710\n",
       "Virat Kohli               IND    707\n",
       "Rohit Sharma              IND    704\n",
       "Kane Williamson            NZ    701"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odi_mens "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1634d41e-81f0-4614-87bf-549ecd6113d3",
   "metadata": {},
   "source": [
    "#### Top 10 Bowler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "a679ab2b-f5ad-40d4-84b8-37a6849506ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Player</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Trent Boult</th>\n",
       "      <td>NZ</td>\n",
       "      <td>760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Josh Hazlewood</th>\n",
       "      <td>AUS</td>\n",
       "      <td>727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mitchell Starc</th>\n",
       "      <td>AUS</td>\n",
       "      <td>665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shaheen Afridi</th>\n",
       "      <td>PAK</td>\n",
       "      <td>661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Matt Henry</th>\n",
       "      <td>NZ</td>\n",
       "      <td>656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adam Zampa</th>\n",
       "      <td>AUS</td>\n",
       "      <td>655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mehedi Hasan</th>\n",
       "      <td>BAN</td>\n",
       "      <td>655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mujeeb Ur Rahman</th>\n",
       "      <td>AFG</td>\n",
       "      <td>650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mustafizur Rahman</th>\n",
       "      <td>BAN</td>\n",
       "      <td>640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rashid Khan</th>\n",
       "      <td>AFG</td>\n",
       "      <td>635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Country Rating\n",
       "Player                          \n",
       "Trent Boult            NZ    760\n",
       "Josh Hazlewood        AUS    727\n",
       "Mitchell Starc        AUS    665\n",
       "Shaheen Afridi        PAK    661\n",
       "Matt Henry             NZ    656\n",
       "Adam Zampa            AUS    655\n",
       "Mehedi Hasan          BAN    655\n",
       "Mujeeb Ur Rahman      AFG    650\n",
       "Mustafizur Rahman     BAN    640\n",
       "Rashid Khan           AFG    635"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = pd.DataFrame([x.text for x in soup.find_all('td', class_ = 'table-body__cell name')],columns = ['Player']).iloc[9:18, :]\n",
    "country = pd.DataFrame([x.text for x in soup.find_all('span', class_ = 'table-body__logo-text')],columns = ['Country']).iloc[9:18, :]\n",
    "rating = pd.DataFrame([x.text for x in soup.find_all('td', class_ = 'table-body__cell u-text-right rating')],columns = ['Rating']).iloc[9:18, :]\n",
    "\n",
    "name = name['Player'].str.split('\\n',expand=True).iloc[:,1:2].rename(columns = {1: 'Player'})\n",
    "\n",
    "mens_bowler = pd.concat([name, country, rating], axis = 1)\n",
    "\n",
    "name = pd.DataFrame([x.text for x in soup.find_all('div', class_ = 'rankings-block__banner--player-info')],columns = ['Player']).iloc[1:2,:]\n",
    "player = name['Player'].str.split('\\n', expand = True).rename(columns = {1:'Player', 4:'Country', 5:'Rating'})\n",
    "player.drop(columns = [2, 3, 6, 7, 0], inplace = True)\n",
    "\n",
    "odi_mens_bowler = pd.concat([player, mens_bowler] , axis = 0)\n",
    "odi_mens_bowler = odi_mens_bowler.set_index('Player')\n",
    "odi_mens_bowler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6f4d45-5f32-46f8-8f16-a44a5923f428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c030d972-8364-4008-9df8-2812fd313bd1",
   "metadata": {},
   "source": [
    "#### 6) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "* a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "* b) Top 10 women’s ODI Batting players along with the records of their team and rating.\n",
    "* c) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b9f8ec-3d0c-4fb4-8716-07066d750cb2",
   "metadata": {},
   "source": [
    "#### Top Ten Women Batsmen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "5b758f22-5942-4021-b8da-e58c1d040964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "odi_2 = requests.get('https://www.icc-cricket.com/rankings/womens/player-rankings/odi')\n",
    "\n",
    "print(odi_2.status_code)\n",
    "soup_2 = BeautifulSoup(odi_2.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "1e308552-af02-4bde-9d61-765460dcd670",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = pd.DataFrame([x.text for x in soup_2.find_all('td', class_ = 'table-body__cell name')],columns = ['Player']).iloc[:9]\n",
    "country = pd.DataFrame([x.text for x in soup_2.find_all('span', class_ = 'table-body__logo-text')],columns = ['Country']).iloc[:9]\n",
    "rating = pd.DataFrame([x.text for x in soup_2.find_all('td', class_ = 'table-body__cell u-text-right rating')],columns = ['Rating']).iloc[:9]\n",
    "\n",
    "name = name['Player'].str.split('\\n',expand=True).iloc[:,1:2].rename(columns = {1: 'Player'})\n",
    "womens_player = pd.concat([name, country, rating], axis = 1)\n",
    "\n",
    "name = pd.DataFrame([x.text for x in soup_2.find_all('div', class_ = 'rankings-block__banner--player-info')],columns = ['Player']).iloc[:1]\n",
    "nm = name['Player'].str.split('\\n', expand = True).rename(columns = {1:'Player', 4:'Country', 5:'Rating'})\n",
    "nm.drop(columns = [2, 3, 6, 7, 0], inplace = True)\n",
    "\n",
    "odi_womens = pd.concat([nm, womens_player] , axis = 0)\n",
    "odi_womens = odi_womens.set_index('Player')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "198dc042-2e30-408e-b64c-5fada5f7dbc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Player</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alyssa Healy</th>\n",
       "      <td>AUS</td>\n",
       "      <td>785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Beth Mooney</th>\n",
       "      <td>AUS</td>\n",
       "      <td>749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Laura Wolvaardt</th>\n",
       "      <td>SA</td>\n",
       "      <td>732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Natalie Sciver</th>\n",
       "      <td>ENG</td>\n",
       "      <td>725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Harmanpreet Kaur</th>\n",
       "      <td>IND</td>\n",
       "      <td>716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Smriti Mandhana</th>\n",
       "      <td>IND</td>\n",
       "      <td>714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Meg Lanning</th>\n",
       "      <td>AUS</td>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rachael Haynes</th>\n",
       "      <td>AUS</td>\n",
       "      <td>701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amy Satterthwaite</th>\n",
       "      <td>NZ</td>\n",
       "      <td>661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chamari Athapaththu</th>\n",
       "      <td>SL</td>\n",
       "      <td>655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Country Rating\n",
       "Player                            \n",
       "Alyssa Healy            AUS    785\n",
       "Beth Mooney             AUS    749\n",
       "Laura Wolvaardt          SA    732\n",
       "Natalie Sciver          ENG    725\n",
       "Harmanpreet Kaur        IND    716\n",
       "Smriti Mandhana         IND    714\n",
       "Meg Lanning             AUS    710\n",
       "Rachael Haynes          AUS    701\n",
       "Amy Satterthwaite        NZ    661\n",
       "Chamari Athapaththu      SL    655"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odi_womens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3babe355-20c4-4287-ab88-cb3b86d9174d",
   "metadata": {},
   "source": [
    "#### Top Ten Women Bowlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "83f8a0b3-97ce-42a0-881f-52c9b55edd9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Player</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Sophie Ecclestone</th>\n",
       "      <td>ENG</td>\n",
       "      <td>739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jess Jonassen</th>\n",
       "      <td>AUS</td>\n",
       "      <td>725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Megan Schutt</th>\n",
       "      <td>AUS</td>\n",
       "      <td>722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shabnim Ismail</th>\n",
       "      <td>SA</td>\n",
       "      <td>722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jhulan Goswami</th>\n",
       "      <td>IND</td>\n",
       "      <td>698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hayley Matthews</th>\n",
       "      <td>WI</td>\n",
       "      <td>671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kate Cross</th>\n",
       "      <td>ENG</td>\n",
       "      <td>657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ayabonga Khaka</th>\n",
       "      <td>SA</td>\n",
       "      <td>634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rajeshwari Gayakwad</th>\n",
       "      <td>IND</td>\n",
       "      <td>617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Marizanne Kapp</th>\n",
       "      <td>SA</td>\n",
       "      <td>598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Country Rating\n",
       "Player                            \n",
       "Sophie Ecclestone       ENG    739\n",
       "Jess Jonassen           AUS    725\n",
       "Megan Schutt            AUS    722\n",
       "Shabnim Ismail           SA    722\n",
       "Jhulan Goswami          IND    698\n",
       "Hayley Matthews          WI    671\n",
       "Kate Cross              ENG    657\n",
       "Ayabonga Khaka           SA    634\n",
       "Rajeshwari Gayakwad     IND    617\n",
       "Marizanne Kapp           SA    598"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = pd.DataFrame([x.text for x in soup_2.find_all('td', class_ = 'table-body__cell name')],columns = ['Player']).iloc[9:18, :]\n",
    "country = pd.DataFrame([x.text for x in soup_2.find_all('span', class_ = 'table-body__logo-text')],columns = ['Country']).iloc[9:18, :]\n",
    "rating = pd.DataFrame([x.text for x in soup_2.find_all('td', class_ = 'table-body__cell u-text-right rating')],columns = ['Rating']).iloc[9:18, :]\n",
    "\n",
    "name = name['Player'].str.split('\\n',expand=True).iloc[:,1:2].rename(columns = {1: 'Player'})\n",
    "\n",
    "womens_bowler = pd.concat([name, country, rating], axis = 1)\n",
    "\n",
    "name = pd.DataFrame([x.text for x in soup_2.find_all('div', class_ = 'rankings-block__banner--player-info')],columns = ['Player']).iloc[1:2,:]\n",
    "player = name['Player'].str.split('\\n', expand = True).rename(columns = {1:'Player', 4:'Country', 5:'Rating'})\n",
    "player.drop(columns = [2, 3, 6, 7, 0], inplace = True)\n",
    "\n",
    "odi_womens_bowler = pd.concat([player, womens_bowler] , axis = 0)\n",
    "odi_womens_bowler = odi_womens_bowler.set_index('Player')\n",
    "odi_womens_bowler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb958d79-a008-47fb-9654-aaa8eabd7e0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd25d7f6-6503-44fe-baac-98f03dc5fefa",
   "metadata": {},
   "source": [
    "#### 7) Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world :\n",
    "* i) Headline\n",
    "* ii) Time\n",
    "* iii) News Link "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "5f290ce1-7928-43d7-b9e9-2557432e75dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnbc = requests.get('https://www.cnbc.com/world/?region=world') # Make a request to a web page with requests module\n",
    "cnbc.status_code #HTTP response status codes indicate whether a specific HTTP request has been successfully completed here,‎200 Successful responses ‎418 I'm a teapot · ‎400 Bad Request · ‎422 Unprocessable Entity\n",
    "soup= BeautifulSoup(cnbc.text, 'html.parser') # pulling data out of HTML and XML files with BeautifulSoup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "3dd1468d-f517-4603-bd2a-59e46ac48ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Time</th>\n",
       "      <th>Links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Parking lots are becoming as important as cars...</td>\n",
       "      <td>5 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/03/parking-lots-b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amazon's cloud unit faces cost-sensitive custo...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/03/aws-faces-cost...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Don’t overlook this health warning on your dec...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/03/dont-overlook-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How electric air taxis could shake up the airl...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/03/how-electric-a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I raised 2 successful CEOs and a doctor. Here'...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/03/i-raised-2-suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Delta pilots would get more than 30% in pay ra...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/03/delta-pilots-w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Men participate less in 401(k) plans than wome...</td>\n",
       "      <td>8 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/03/men-participat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The best U.S. states to raise a family if you ...</td>\n",
       "      <td>8 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/03/best-states-ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Susan Cain: This Bob Dylan-inspired phrase can...</td>\n",
       "      <td>8 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/03/bestselling-au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The difference between this comeback and the m...</td>\n",
       "      <td>8 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/03/the-difference...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Goldman says buy these five stocks for the lon...</td>\n",
       "      <td>8 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/03/goldman-says-b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Celsius users with crypto collateral stuck tur...</td>\n",
       "      <td>9 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/03/celsius-users-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Cramer's lightning round: Let Extreme Networks...</td>\n",
       "      <td>21 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/cramers-lightn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Jim Cramer says these 3 apparel stocks benefit...</td>\n",
       "      <td>21 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/jim-cramer-say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Cramer’s week ahead: Markets need a strong job...</td>\n",
       "      <td>22 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/cramers-week-a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Pro Picks: Watch all of Friday's big stock cal...</td>\n",
       "      <td>23 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/pro-picks-watc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>There is 'enormous opportunity' in REITs, says...</td>\n",
       "      <td>23 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/reits-offer-en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Biden administration will end monkeypox public...</td>\n",
       "      <td>December 2, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/biden-administ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Expect more choppiness ahead after a week of m...</td>\n",
       "      <td>December 2, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/expect-more-ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>GM, LG investing $275 million to expand Tennes...</td>\n",
       "      <td>December 2, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/gm-lg-investin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>These beat-up tech stocks have potential, ‘Hal...</td>\n",
       "      <td>December 2, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/big-tech-stock...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>The Fed's path to a 'Goldilocks' economy just ...</td>\n",
       "      <td>December 2, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/the-feds-path-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3 things crypto investors need to know in post...</td>\n",
       "      <td>December 2, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/three-things-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Biden condemns antisemitism as Ye praises Hitl...</td>\n",
       "      <td>December 2, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/biden-condemns...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Georgia man arrested for shooting boy campaign...</td>\n",
       "      <td>December 2, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/georgia-electi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>What to watch in the markets in the week ahead</td>\n",
       "      <td>December 2, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/markets-lookin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>The biggest tax changes to know before filing ...</td>\n",
       "      <td>December 2, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/the-biggest-ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>'This is a crisis.' Why more workers need acce...</td>\n",
       "      <td>December 2, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/why-more-worke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Carnival’s Princess Cruises will return to Jap...</td>\n",
       "      <td>December 2, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/carnivals-prin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Tech layoffs may not be a bad omen for U.S. ec...</td>\n",
       "      <td>December 2, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/tech-layoffs-m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title              Time  \\\n",
       "0   Parking lots are becoming as important as cars...       5 Hours Ago   \n",
       "1   Amazon's cloud unit faces cost-sensitive custo...       7 Hours Ago   \n",
       "2   Don’t overlook this health warning on your dec...       7 Hours Ago   \n",
       "3   How electric air taxis could shake up the airl...       7 Hours Ago   \n",
       "4   I raised 2 successful CEOs and a doctor. Here'...       7 Hours Ago   \n",
       "5   Delta pilots would get more than 30% in pay ra...       7 Hours Ago   \n",
       "6   Men participate less in 401(k) plans than wome...       8 Hours Ago   \n",
       "7   The best U.S. states to raise a family if you ...       8 Hours Ago   \n",
       "8   Susan Cain: This Bob Dylan-inspired phrase can...       8 Hours Ago   \n",
       "9   The difference between this comeback and the m...       8 Hours Ago   \n",
       "10  Goldman says buy these five stocks for the lon...       8 Hours Ago   \n",
       "11  Celsius users with crypto collateral stuck tur...       9 Hours Ago   \n",
       "12  Cramer's lightning round: Let Extreme Networks...      21 Hours Ago   \n",
       "13  Jim Cramer says these 3 apparel stocks benefit...      21 Hours Ago   \n",
       "14  Cramer’s week ahead: Markets need a strong job...      22 Hours Ago   \n",
       "15  Pro Picks: Watch all of Friday's big stock cal...      23 Hours Ago   \n",
       "16  There is 'enormous opportunity' in REITs, says...      23 Hours Ago   \n",
       "17  Biden administration will end monkeypox public...  December 2, 2022   \n",
       "18  Expect more choppiness ahead after a week of m...  December 2, 2022   \n",
       "19  GM, LG investing $275 million to expand Tennes...  December 2, 2022   \n",
       "20  These beat-up tech stocks have potential, ‘Hal...  December 2, 2022   \n",
       "21  The Fed's path to a 'Goldilocks' economy just ...  December 2, 2022   \n",
       "22  3 things crypto investors need to know in post...  December 2, 2022   \n",
       "23  Biden condemns antisemitism as Ye praises Hitl...  December 2, 2022   \n",
       "24  Georgia man arrested for shooting boy campaign...  December 2, 2022   \n",
       "25     What to watch in the markets in the week ahead  December 2, 2022   \n",
       "26  The biggest tax changes to know before filing ...  December 2, 2022   \n",
       "27  'This is a crisis.' Why more workers need acce...  December 2, 2022   \n",
       "28  Carnival’s Princess Cruises will return to Jap...  December 2, 2022   \n",
       "29  Tech layoffs may not be a bad omen for U.S. ec...  December 2, 2022   \n",
       "\n",
       "                                                Links  \n",
       "0   https://www.cnbc.com/2022/12/03/parking-lots-b...  \n",
       "1   https://www.cnbc.com/2022/12/03/aws-faces-cost...  \n",
       "2   https://www.cnbc.com/2022/12/03/dont-overlook-...  \n",
       "3   https://www.cnbc.com/2022/12/03/how-electric-a...  \n",
       "4   https://www.cnbc.com/2022/12/03/i-raised-2-suc...  \n",
       "5   https://www.cnbc.com/2022/12/03/delta-pilots-w...  \n",
       "6   https://www.cnbc.com/2022/12/03/men-participat...  \n",
       "7   https://www.cnbc.com/2022/12/03/best-states-ra...  \n",
       "8   https://www.cnbc.com/2022/12/03/bestselling-au...  \n",
       "9   https://www.cnbc.com/2022/12/03/the-difference...  \n",
       "10  https://www.cnbc.com/2022/12/03/goldman-says-b...  \n",
       "11  https://www.cnbc.com/2022/12/03/celsius-users-...  \n",
       "12  https://www.cnbc.com/2022/12/02/cramers-lightn...  \n",
       "13  https://www.cnbc.com/2022/12/02/jim-cramer-say...  \n",
       "14  https://www.cnbc.com/2022/12/02/cramers-week-a...  \n",
       "15  https://www.cnbc.com/2022/12/02/pro-picks-watc...  \n",
       "16  https://www.cnbc.com/2022/12/02/reits-offer-en...  \n",
       "17  https://www.cnbc.com/2022/12/02/biden-administ...  \n",
       "18  https://www.cnbc.com/2022/12/02/expect-more-ch...  \n",
       "19  https://www.cnbc.com/2022/12/02/gm-lg-investin...  \n",
       "20  https://www.cnbc.com/2022/12/02/big-tech-stock...  \n",
       "21  https://www.cnbc.com/2022/12/02/the-feds-path-...  \n",
       "22  https://www.cnbc.com/2022/12/02/three-things-c...  \n",
       "23  https://www.cnbc.com/2022/12/02/biden-condemns...  \n",
       "24  https://www.cnbc.com/2022/12/02/georgia-electi...  \n",
       "25  https://www.cnbc.com/2022/12/02/markets-lookin...  \n",
       "26  https://www.cnbc.com/2022/12/02/the-biggest-ta...  \n",
       "27  https://www.cnbc.com/2022/12/02/why-more-worke...  \n",
       "28  https://www.cnbc.com/2022/12/02/carnivals-prin...  \n",
       "29  https://www.cnbc.com/2022/12/02/tech-layoffs-m...  "
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_tag = soup.find_all('a', class_ = 'LatestNews-headline') \n",
    "title = pd.DataFrame([x.text for x in a_tag], columns = ['Title'])\n",
    "\n",
    "time = soup.find_all('time', class_ = 'LatestNews-timestamp')\n",
    "Time = pd.DataFrame([x.text for x in time], columns = ['Time'])\n",
    "\n",
    "links = pd.DataFrame([link.get(\"href\") for link in soup(\"a\", class_ = 'LatestNews-headline')], columns = ['Links'])\n",
    "\n",
    "data_cnbc = pd.concat([title, Time, links], axis = 1)\n",
    "data_cnbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38577628-6d10-4493-92da-e5900437fa83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7aa80e88-7655-4d7f-bc43-fcb67642dd28",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 8) Write a python program to scrape the details of most downloaded articles from AI in last 90 days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "0896afb2-10a4-4506-889e-64e2f47ec9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "AI = requests.get('https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles') # Make a request to a web page with requests module\n",
    "# HTML parser is a structured markup processing tool\n",
    "print(AI.status_code)  #HTTP response status codes indicate whether a specific HTTP request has been successfully completed here,‎200 Successful responses ‎418 I'm a teapot · ‎400 Bad Request · ‎422 Unprocessable Entity\n",
    "soup = BeautifulSoup(AI.text, 'html.parser')  # pulling data out of HTML and XML files with BeautifulSoup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "3abb3f7a-747a-4c5a-a51b-c5f491491f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paper_Title</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Published_Date</th>\n",
       "      <th>Paper_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reward is enough</td>\n",
       "      <td>Silver, David, Singh, Satinder, Precup, Doina,...</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Making sense of raw input</td>\n",
       "      <td>Evans, Richard, Bošnjak, Matko and 5 more</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Law and logic: A review from an argumentation ...</td>\n",
       "      <td>Prakken, Henry, Sartor, Giovanni</td>\n",
       "      <td>October 2015</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Creativity and artificial intelligence</td>\n",
       "      <td>Boden, Margaret A.</td>\n",
       "      <td>August 1998</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Artificial cognition for social human–robot in...</td>\n",
       "      <td>Lemaignan, Séverin, Warnier, Mathieu and 3 more</td>\n",
       "      <td>June 2017</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Explanation in artificial intelligence: Insigh...</td>\n",
       "      <td>Miller, Tim</td>\n",
       "      <td>February 2019</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Making sense of sensory input</td>\n",
       "      <td>Evans, Richard, Hernández-Orallo, José and 3 more</td>\n",
       "      <td>April 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Conflict-based search for optimal multi-agent ...</td>\n",
       "      <td>Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...</td>\n",
       "      <td>February 2015</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Between MDPs and semi-MDPs: A framework for te...</td>\n",
       "      <td>Sutton, Richard S., Precup, Doina, Singh, Sati...</td>\n",
       "      <td>August 1999</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Hanabi challenge: A new frontier for AI re...</td>\n",
       "      <td>Bard, Nolan, Foerster, Jakob N. and 13 more</td>\n",
       "      <td>March 2020</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Evaluating XAI: A comparison of rule-based and...</td>\n",
       "      <td>van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...</td>\n",
       "      <td>February 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Argumentation in artificial intelligence</td>\n",
       "      <td>Bench-Capon, T.J.M., Dunne, Paul E.</td>\n",
       "      <td>October 2007</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Algorithms for computing strategies in two-pla...</td>\n",
       "      <td>Bošanský, Branislav, Lisý, Viliam and 3 more</td>\n",
       "      <td>August 2016</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Multiple object tracking: A literature review</td>\n",
       "      <td>Luo, Wenhan, Xing, Junliang and 4 more</td>\n",
       "      <td>April 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Selection of relevant features and examples in...</td>\n",
       "      <td>Blum, Avrim L., Langley, Pat</td>\n",
       "      <td>December 1997</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>A survey of inverse reinforcement learning: Ch...</td>\n",
       "      <td>Arora, Saurabh, Doshi, Prashant</td>\n",
       "      <td>August 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Explaining individual predictions when feature...</td>\n",
       "      <td>Aas, Kjersti, Jullum, Martin, Løland, Anders</td>\n",
       "      <td>September 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>A review of possible effects of cognitive bias...</td>\n",
       "      <td>Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...</td>\n",
       "      <td>June 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Integrating social power into the decision-mak...</td>\n",
       "      <td>Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.</td>\n",
       "      <td>December 2016</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>“That's (not) the output I expected!” On the r...</td>\n",
       "      <td>Riveiro, Maria, Thill, Serge</td>\n",
       "      <td>September 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Explaining black-box classifiers using post-ho...</td>\n",
       "      <td>Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...</td>\n",
       "      <td>May 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Algorithm runtime prediction: Methods &amp; evalua...</td>\n",
       "      <td>Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...</td>\n",
       "      <td>January 2014</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Wrappers for feature subset selection</td>\n",
       "      <td>Kohavi, Ron, John, George H.</td>\n",
       "      <td>December 1997</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Commonsense visual sensemaking for autonomous ...</td>\n",
       "      <td>Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Quantum computation, quantum theory and AI</td>\n",
       "      <td>Ying, Mingsheng</td>\n",
       "      <td>February 2010</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Paper_Title  \\\n",
       "0                                    Reward is enough   \n",
       "1                           Making sense of raw input   \n",
       "2   Law and logic: A review from an argumentation ...   \n",
       "3              Creativity and artificial intelligence   \n",
       "4   Artificial cognition for social human–robot in...   \n",
       "5   Explanation in artificial intelligence: Insigh...   \n",
       "6                       Making sense of sensory input   \n",
       "7   Conflict-based search for optimal multi-agent ...   \n",
       "8   Between MDPs and semi-MDPs: A framework for te...   \n",
       "9   The Hanabi challenge: A new frontier for AI re...   \n",
       "10  Evaluating XAI: A comparison of rule-based and...   \n",
       "11           Argumentation in artificial intelligence   \n",
       "12  Algorithms for computing strategies in two-pla...   \n",
       "13      Multiple object tracking: A literature review   \n",
       "14  Selection of relevant features and examples in...   \n",
       "15  A survey of inverse reinforcement learning: Ch...   \n",
       "16  Explaining individual predictions when feature...   \n",
       "17  A review of possible effects of cognitive bias...   \n",
       "18  Integrating social power into the decision-mak...   \n",
       "19  “That's (not) the output I expected!” On the r...   \n",
       "20  Explaining black-box classifiers using post-ho...   \n",
       "21  Algorithm runtime prediction: Methods & evalua...   \n",
       "22              Wrappers for feature subset selection   \n",
       "23  Commonsense visual sensemaking for autonomous ...   \n",
       "24         Quantum computation, quantum theory and AI   \n",
       "\n",
       "                                              Authors  Published_Date  \\\n",
       "0   Silver, David, Singh, Satinder, Precup, Doina,...    October 2021   \n",
       "1           Evans, Richard, Bošnjak, Matko and 5 more    October 2021   \n",
       "2                   Prakken, Henry, Sartor, Giovanni     October 2015   \n",
       "3                                 Boden, Margaret A.      August 1998   \n",
       "4     Lemaignan, Séverin, Warnier, Mathieu and 3 more       June 2017   \n",
       "5                                        Miller, Tim    February 2019   \n",
       "6   Evans, Richard, Hernández-Orallo, José and 3 more      April 2021   \n",
       "7   Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...   February 2015   \n",
       "8   Sutton, Richard S., Precup, Doina, Singh, Sati...     August 1999   \n",
       "9         Bard, Nolan, Foerster, Jakob N. and 13 more      March 2020   \n",
       "10  van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...   February 2021   \n",
       "11               Bench-Capon, T.J.M., Dunne, Paul E.     October 2007   \n",
       "12       Bošanský, Branislav, Lisý, Viliam and 3 more     August 2016   \n",
       "13             Luo, Wenhan, Xing, Junliang and 4 more      April 2021   \n",
       "14                      Blum, Avrim L., Langley, Pat    December 1997   \n",
       "15                   Arora, Saurabh, Doshi, Prashant      August 2021   \n",
       "16      Aas, Kjersti, Jullum, Martin, Løland, Anders   September 2021   \n",
       "17  Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...       June 2021   \n",
       "18    Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.    December 2016   \n",
       "19                      Riveiro, Maria, Thill, Serge   September 2021   \n",
       "20  Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...        May 2021   \n",
       "21  Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...    January 2014   \n",
       "22                      Kohavi, Ron, John, George H.    December 1997   \n",
       "23  Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...    October 2021   \n",
       "24                                   Ying, Mingsheng    February 2010   \n",
       "\n",
       "                                            Paper_url  \n",
       "0   https://www.sciencedirect.com/science/article/...  \n",
       "1   https://www.sciencedirect.com/science/article/...  \n",
       "2   https://www.sciencedirect.com/science/article/...  \n",
       "3   https://www.sciencedirect.com/science/article/...  \n",
       "4   https://www.sciencedirect.com/science/article/...  \n",
       "5   https://www.sciencedirect.com/science/article/...  \n",
       "6   https://www.sciencedirect.com/science/article/...  \n",
       "7   https://www.sciencedirect.com/science/article/...  \n",
       "8   https://www.sciencedirect.com/science/article/...  \n",
       "9   https://www.sciencedirect.com/science/article/...  \n",
       "10  https://www.sciencedirect.com/science/article/...  \n",
       "11  https://www.sciencedirect.com/science/article/...  \n",
       "12  https://www.sciencedirect.com/science/article/...  \n",
       "13  https://www.sciencedirect.com/science/article/...  \n",
       "14  https://www.sciencedirect.com/science/article/...  \n",
       "15  https://www.sciencedirect.com/science/article/...  \n",
       "16  https://www.sciencedirect.com/science/article/...  \n",
       "17  https://www.sciencedirect.com/science/article/...  \n",
       "18  https://www.sciencedirect.com/science/article/...  \n",
       "19  https://www.sciencedirect.com/science/article/...  \n",
       "20  https://www.sciencedirect.com/science/article/...  \n",
       "21  https://www.sciencedirect.com/science/article/...  \n",
       "22  https://www.sciencedirect.com/science/article/...  \n",
       "23  https://www.sciencedirect.com/science/article/...  \n",
       "24  https://www.sciencedirect.com/science/article/...  "
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = soup.find_all('h2', class_ = 'sc-1qrq3sd-1 gRGSUS sc-1nmom32-0 sc-1nmom32-1 btcbYu goSKRg') # find all h2 tags with specific class mentioned\n",
    "title = pd.DataFrame([x.text for x in title], columns = ['Paper_Title']) # x.text is for print the response text and convert it into the Dataframe with od.DataFrame\n",
    "\n",
    "author = soup.find_all('span', class_ = 'sc-1w3fpd7-0 dnCnAO')   #find all span tags with specific class mentioned\n",
    "author = pd.DataFrame([x.text for x in author], columns = ['Authors']) # x.text is for print the response text and convert it into the Dataframe with od.DataFrame\n",
    "\n",
    "date = soup.find_all('span', class_ = 'sc-1thf9ly-2 dvggWt') # find all span tags with specific class mentioned\n",
    "date=pd.DataFrame([x.text for x in date], columns = ['Published_Date'])  # x.text is for print the response text and convert it into the Dataframe with od.DataFrame\n",
    "\n",
    "links = pd.DataFrame([link.get('href') for link in soup('a', class_ = 'sc-5smygv-0 fIXTHm')], columns = ['Paper_url']) # get perticular class links and convert into dataframe\n",
    "AI_data = pd.concat([title, author, date, links], axis = 1)   # concat(aggregate) all the columns in one data with concat function\n",
    "AI_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c266e611-d00b-453e-a78a-777dc2b29387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb46cca6-52a4-4375-af29-23bf9b99e613",
   "metadata": {},
   "source": [
    "#### 9) Write a python program to scrape mentioned details from dineout.co.in :\n",
    "* i) Restaurant name\n",
    "* ii) Cuisine\n",
    "* iii) Location\n",
    "* iv) Ratings\n",
    "* v) Image URL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "89b46038-8264-4774-b41b-aa9240e1c247",
   "metadata": {},
   "outputs": [],
   "source": [
    "dine = requests.get('https://www.dineout.co.in/mumbai-restaurants/10instant-discount') # Make a request to a web page with requests module\n",
    "# pulling data out of HTML and XML files with BeautifulSoup\n",
    "soup = BeautifulSoup(dine.text, 'html.parser')  #HTTP response status codes indicate whether a specific HTTP request has been successfully completed here,‎200 Successful responses ‎418 I'm a teapot · ‎400 Bad Request · ‎422 Unprocessable Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "a7943501-98a4-453c-9c6c-092068641d79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant_Name</th>\n",
       "      <th>Address</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Image_Url</th>\n",
       "      <th>Cuisine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eve</td>\n",
       "      <td>Sentinel Building,Powai, Powai</td>\n",
       "      <td>4.1</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "      <td>Asian, Continental, Italian, American, Sushi,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sigree Global Grill</td>\n",
       "      <td>Ventura Building,Powai, Powai</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "      <td>North Indian, Biryani</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Independence Brewing Company</td>\n",
       "      <td>Hiranandani, Powai</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "      <td>Continental, Finger Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pop Tate's</td>\n",
       "      <td>Mayfair Sonata Green CHS,Vikhroli West, Centra...</td>\n",
       "      <td>4.4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "      <td>Continental, Chinese, Fast Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pop Tate's</td>\n",
       "      <td>R City Mall,Ghatkopar West, Central Suburbs</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "      <td>Continental, Chinese, Fast Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Urban Tadka</td>\n",
       "      <td>R City Mall,Ghatkopar West, Central Suburbs</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "      <td>North Indian, Mughlai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Cream Centre- Ghatkopar West</td>\n",
       "      <td>R City Mall,Ghatkopar West, Central Suburbs</td>\n",
       "      <td>4.2</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "      <td>North Indian, Fast Food, Italian, Mexican, Be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Khandani Rajdhani</td>\n",
       "      <td>R City Mall,Ghatkopar West, Central Suburbs</td>\n",
       "      <td>4.4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "      <td>Gujarati, Rajasthani, North Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Kake Da Hotel</td>\n",
       "      <td>Powai, Powai</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "      <td>Chinese, North Indian, Biryani</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Timbuctoo</td>\n",
       "      <td>Mayfair Sonata Green CHS,Vikhroli West, Centra...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "      <td>Finger Food, Chinese, Italian, North Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Origami Japanese And Korean Restaurant</td>\n",
       "      <td>Supreme Business Park,Powai, Powai</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "      <td>Japanese, Korean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Sbarro - New York Pizza</td>\n",
       "      <td>Kailash Business Park,Vikhroli West, Central S...</td>\n",
       "      <td>4.2</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "      <td>Pizza, Fast Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Malgudi</td>\n",
       "      <td>R City Mall,Ghatkopar West, Central Suburbs</td>\n",
       "      <td>4.1</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "      <td>South Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The Game Ranch</td>\n",
       "      <td>Delphi Building,Powai, Powai</td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "      <td>Continental, Mexican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Coco Cafe</td>\n",
       "      <td>Ghatkopar West, Central Suburbs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "      <td>Continental, Desserts, Beverages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Mad Over Donuts</td>\n",
       "      <td>City Park Building,Powai, Powai</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "      <td>Desserts, Beverages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Street Foods By Punjab Grill</td>\n",
       "      <td>R City Mall,Ghatkopar West, Central Suburbs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "      <td>Mughlai, North Indian, Street Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Cogs and Brews Cafe</td>\n",
       "      <td>Powai, Powai</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "      <td>Fast Food, Desserts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Mad Over Donuts</td>\n",
       "      <td>R City Mall,Ghatkopar West, Central Suburbs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "      <td>Desserts, Beverages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Sbarro - New York Pizza</td>\n",
       "      <td>R City Mall,Ghatkopar West, Central Suburbs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "      <td>Pizza, Fast Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Mini Punjab's Lake Side</td>\n",
       "      <td>Powai, Powai</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "      <td>Mughlai, North Indian, Chinese, Seafood</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Restaurant_Name  \\\n",
       "0                                      Eve   \n",
       "1                      Sigree Global Grill   \n",
       "2             Independence Brewing Company   \n",
       "3                               Pop Tate's   \n",
       "4                               Pop Tate's   \n",
       "5                              Urban Tadka   \n",
       "6             Cream Centre- Ghatkopar West   \n",
       "7                        Khandani Rajdhani   \n",
       "8                            Kake Da Hotel   \n",
       "9                                Timbuctoo   \n",
       "10  Origami Japanese And Korean Restaurant   \n",
       "11                 Sbarro - New York Pizza   \n",
       "12                                 Malgudi   \n",
       "13                          The Game Ranch   \n",
       "14                               Coco Cafe   \n",
       "15                         Mad Over Donuts   \n",
       "16            Street Foods By Punjab Grill   \n",
       "17                     Cogs and Brews Cafe   \n",
       "18                         Mad Over Donuts   \n",
       "19                 Sbarro - New York Pizza   \n",
       "20                 Mini Punjab's Lake Side   \n",
       "\n",
       "                                              Address Ratings  \\\n",
       "0                      Sentinel Building,Powai, Powai     4.1   \n",
       "1                       Ventura Building,Powai, Powai     4.3   \n",
       "2                                  Hiranandani, Powai     4.3   \n",
       "3   Mayfair Sonata Green CHS,Vikhroli West, Centra...     4.4   \n",
       "4         R City Mall,Ghatkopar West, Central Suburbs     4.3   \n",
       "5         R City Mall,Ghatkopar West, Central Suburbs     4.3   \n",
       "6         R City Mall,Ghatkopar West, Central Suburbs     4.2   \n",
       "7         R City Mall,Ghatkopar West, Central Suburbs     4.4   \n",
       "8                                        Powai, Powai     4.3   \n",
       "9   Mayfair Sonata Green CHS,Vikhroli West, Centra...     4.3   \n",
       "10                 Supreme Business Park,Powai, Powai     3.9   \n",
       "11  Kailash Business Park,Vikhroli West, Central S...     4.2   \n",
       "12        R City Mall,Ghatkopar West, Central Suburbs     4.1   \n",
       "13                       Delphi Building,Powai, Powai       4   \n",
       "14                    Ghatkopar West, Central Suburbs     NaN   \n",
       "15                    City Park Building,Powai, Powai     NaN   \n",
       "16        R City Mall,Ghatkopar West, Central Suburbs     NaN   \n",
       "17                                       Powai, Powai     NaN   \n",
       "18        R City Mall,Ghatkopar West, Central Suburbs     NaN   \n",
       "19        R City Mall,Ghatkopar West, Central Suburbs     NaN   \n",
       "20                                       Powai, Powai     NaN   \n",
       "\n",
       "                                            Image_Url  \\\n",
       "0   https://im1.dineout.co.in/images/uploads/resta...   \n",
       "1   https://im1.dineout.co.in/images/uploads/resta...   \n",
       "2   https://im1.dineout.co.in/images/uploads/resta...   \n",
       "3   https://im1.dineout.co.in/images/uploads/resta...   \n",
       "4   https://im1.dineout.co.in/images/uploads/resta...   \n",
       "5   https://im1.dineout.co.in/images/uploads/resta...   \n",
       "6   https://im1.dineout.co.in/images/uploads/resta...   \n",
       "7   https://im1.dineout.co.in/images/uploads/resta...   \n",
       "8   https://im1.dineout.co.in/images/uploads/resta...   \n",
       "9   https://im1.dineout.co.in/images/uploads/resta...   \n",
       "10  https://im1.dineout.co.in/images/uploads/resta...   \n",
       "11  https://im1.dineout.co.in/images/uploads/resta...   \n",
       "12  https://im1.dineout.co.in/images/uploads/resta...   \n",
       "13  https://im1.dineout.co.in/images/uploads/resta...   \n",
       "14  https://im1.dineout.co.in/images/uploads/resta...   \n",
       "15  https://im1.dineout.co.in/images/uploads/resta...   \n",
       "16  https://im1.dineout.co.in/images/uploads/resta...   \n",
       "17  https://im1.dineout.co.in/images/uploads/resta...   \n",
       "18  https://im1.dineout.co.in/images/uploads/resta...   \n",
       "19  https://im1.dineout.co.in/images/uploads/resta...   \n",
       "20  https://im1.dineout.co.in/images/uploads/resta...   \n",
       "\n",
       "                                              Cuisine  \n",
       "0    Asian, Continental, Italian, American, Sushi,...  \n",
       "1                               North Indian, Biryani  \n",
       "2                            Continental, Finger Food  \n",
       "3                     Continental, Chinese, Fast Food  \n",
       "4                     Continental, Chinese, Fast Food  \n",
       "5                               North Indian, Mughlai  \n",
       "6    North Indian, Fast Food, Italian, Mexican, Be...  \n",
       "7                  Gujarati, Rajasthani, North Indian  \n",
       "8                      Chinese, North Indian, Biryani  \n",
       "9         Finger Food, Chinese, Italian, North Indian  \n",
       "10                                   Japanese, Korean  \n",
       "11                                   Pizza, Fast Food  \n",
       "12                                       South Indian  \n",
       "13                               Continental, Mexican  \n",
       "14                   Continental, Desserts, Beverages  \n",
       "15                                Desserts, Beverages  \n",
       "16                 Mughlai, North Indian, Street Food  \n",
       "17                                Fast Food, Desserts  \n",
       "18                                Desserts, Beverages  \n",
       "19                                   Pizza, Fast Food  \n",
       "20            Mughlai, North Indian, Chinese, Seafood  "
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = pd.DataFrame([x.text for x in soup.find_all('a', class_ = 'restnt-name ellipsis')], columns = ['Restaurant_Name']) \n",
    "cuisine=pd.DataFrame([x.text for x in soup.find_all('span', class_ = 'double-line-ellipsis')], columns = ['Cuisine'])\n",
    "location = pd.DataFrame([x.text for x in soup.find_all('div', class_ = 'restnt-loc ellipsis')], columns = ['Address'])\n",
    "rating = pd.DataFrame([x.text for x in soup.find_all('div', class_ = 'restnt-rating rating-4')], columns = ['Ratings'])\n",
    "link = pd.DataFrame([x.get('data-src') for x in soup.find_all('img', class_ = 'no-img')], columns = ['Image_Url'])\n",
    "\n",
    "data = pd.concat([name, location, rating, link, cuisine], axis = 1)\n",
    "Cuisine = data['Cuisine'].str.split('|', expand=True)\n",
    "Cuisine.rename(columns = {1:'Cuisine'}, inplace = True)\n",
    "data = data.iloc[:,:4]\n",
    "data = pd.concat([data, Cuisine], axis = 1).rename(columns = {0:'Price'})\n",
    "data.drop('Price', axis = 1, inplace = True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c29d8e4-e032-4c90-ae0e-7f97656a5d04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00055bee-7892-4ffb-92ef-071f05238599",
   "metadata": {},
   "source": [
    "#### 10) Write a python program to scrape the details of top publications from Google Scholar from https://scholar.google.com/citations?view_op=top_venues&hl=en\n",
    "* i) Rank\n",
    "* ii) Publication\n",
    "* iii) h5-index\n",
    "* iv) h5-median "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "2d07808c-476d-4cd0-a48d-6cdfb57341d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dine = requests.get('https://scholar.google.com/citations?view_op=top_venues&hl=en') # Make a request to a web page with requests module\n",
    "dine.status_code #HTTP response status codes indicate whether a specific HTTP request has been successfully completed here,‎200 Successful responses ‎418 I'm a teapot · ‎400 Bad Request · ‎422 Unprocessable Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "a44a9946-233c-4c04-bcb0-6c54d0a19723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>h5_median</th>\n",
       "      <th>h5_index</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.</th>\n",
       "      <td>Nature</td>\n",
       "      <td>667</td>\n",
       "      <td>444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.</th>\n",
       "      <td>The New England Journal of Medicine</td>\n",
       "      <td>780</td>\n",
       "      <td>432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.</th>\n",
       "      <td>Science</td>\n",
       "      <td>614</td>\n",
       "      <td>401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.</th>\n",
       "      <td>IEEE/CVF Conference on Computer Vision and Pat...</td>\n",
       "      <td>627</td>\n",
       "      <td>389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.</th>\n",
       "      <td>The Lancet</td>\n",
       "      <td>635</td>\n",
       "      <td>354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96.</th>\n",
       "      <td>Journal of Business Research</td>\n",
       "      <td>233</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97.</th>\n",
       "      <td>Molecular Cancer</td>\n",
       "      <td>209</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98.</th>\n",
       "      <td>Sensors</td>\n",
       "      <td>201</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99.</th>\n",
       "      <td>Nature Climate Change</td>\n",
       "      <td>228</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100.</th>\n",
       "      <td>IEEE Internet of Things Journal</td>\n",
       "      <td>212</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Name h5_median h5_index\n",
       "rank                                                                      \n",
       "1.                                               Nature       667      444\n",
       "2.                  The New England Journal of Medicine       780      432\n",
       "3.                                              Science       614      401\n",
       "4.    IEEE/CVF Conference on Computer Vision and Pat...       627      389\n",
       "5.                                           The Lancet       635      354\n",
       "...                                                 ...       ...      ...\n",
       "96.                        Journal of Business Research       233      145\n",
       "97.                                    Molecular Cancer       209      145\n",
       "98.                                             Sensors       201      145\n",
       "99.                               Nature Climate Change       228      144\n",
       "100.                    IEEE Internet of Things Journal       212      144\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(dine.text, 'html.parser') \n",
    "\n",
    "name = soup.find_all('td', class_ = 'gsc_mvt_t')\n",
    "name = pd.DataFrame([x.text for x in name], columns = ['Name'])\n",
    "\n",
    "index = soup.find_all('a', class_='gs_ibl gsc_mp_anchor')\n",
    "\n",
    "h5_index = pd.DataFrame([x.text for x in index], columns = ['h5_index'])\n",
    "h5 = soup.find_all('span', class_ = 'gs_ibl gsc_mp_anchor')\n",
    "h5 = pd.DataFrame([x.text for x in h5], columns = ['h5_median'])\n",
    "\n",
    "rank = soup.find_all('td', class_ = 'gsc_mvt_p')\n",
    "rank = pd.DataFrame([x.text for x in rank], columns = ['rank'])\n",
    "\n",
    "google_scholar = pd.concat([rank, name, h5, h5_index ], axis = 1)\n",
    "\n",
    "google_scholar = google_scholar.set_index('rank')\n",
    "google_scholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4f0e57-8b90-46b6-8356-1484e9397fa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
